{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Methods       | Accuracy         | \n",
    "| ------------- |:-------------:| \n",
    "| TF-IDF + Random Forest    | 0.39| \n",
    "| Word2Vec + Random Forest    | 0.52     |\n",
    "| Word2Vec + Neural Network | 0.48     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156690, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('./chinese_restaurants_review.json', lines=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cool</th>\n",
       "      <th>funny</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This place is a gem. We had friendly attentive...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This is perhaps the closest pho restaurant to ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Happened to stumble upon this little quaint re...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Very good food for decent prices. The atmosphe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>The food was very fresh and quite tasty. The m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cool  funny                                               text  stars\n",
       "0     0      0  This place is a gem. We had friendly attentive...      5\n",
       "1     0      0  This is perhaps the closest pho restaurant to ...      5\n",
       "2     0      0  Happened to stumble upon this little quaint re...      4\n",
       "3     0      0  Very good food for decent prices. The atmosphe...      4\n",
       "4     1      1  The food was very fresh and quite tasty. The m...      3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select columns we need\n",
    "data = data[['cool', 'funny','text','stars']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get token and stemming, also remove punctuation\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "ps = PorterStemmer()\n",
    "def token(x):\n",
    "    \n",
    "    x = x.lower()\n",
    "    token = nltk.word_tokenize(x)\n",
    "    \n",
    "    # remove stop words\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    token = [x for x in token if x not in stopWords]\n",
    "    \n",
    "    # transform each token into string and remove punctuation\n",
    "    t = [x.encode('utf-8').translate(None, string.punctuation) for x in token ]\n",
    "    \n",
    "    # after removing punctuation, some token may become ' '\n",
    "    return [x for x in t if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total time spent:', datetime.timedelta(0, 339, 55161))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cool</th>\n",
       "      <th>funny</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This place is a gem. We had friendly attentive...</td>\n",
       "      <td>5</td>\n",
       "      <td>[place, gem, friendly, attentive, service, foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This is perhaps the closest pho restaurant to ...</td>\n",
       "      <td>5</td>\n",
       "      <td>[perhaps, closest, pho, restaurant, port, cred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Happened to stumble upon this little quaint re...</td>\n",
       "      <td>4</td>\n",
       "      <td>[happened, stumble, upon, little, quaint, rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Very good food for decent prices. The atmosphe...</td>\n",
       "      <td>4</td>\n",
       "      <td>[good, food, decent, prices, atmosphere, nice,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>The food was very fresh and quite tasty. The m...</td>\n",
       "      <td>3</td>\n",
       "      <td>[food, fresh, quite, tasty, mango, salad, nice...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cool  funny                                               text  stars  \\\n",
       "0     0      0  This place is a gem. We had friendly attentive...      5   \n",
       "1     0      0  This is perhaps the closest pho restaurant to ...      5   \n",
       "2     0      0  Happened to stumble upon this little quaint re...      4   \n",
       "3     0      0  Very good food for decent prices. The atmosphe...      4   \n",
       "4     1      1  The food was very fresh and quite tasty. The m...      3   \n",
       "\n",
       "                                               token  \n",
       "0  [place, gem, friendly, attentive, service, foo...  \n",
       "1  [perhaps, closest, pho, restaurant, port, cred...  \n",
       "2  [happened, stumble, upon, little, quaint, rest...  \n",
       "3  [good, food, decent, prices, atmosphere, nice,...  \n",
       "4  [food, fresh, quite, tasty, mango, salad, nice...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin = datetime.datetime.now()\n",
    "data['token'] = data.text.apply(lambda x: token(x))\n",
    "print ('Total time spent:', datetime.datetime.now() - begin)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "num_features = 300   # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10         # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total time spent:', 98.15893697738647)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "begin = time.time()\n",
    "w2v_model = word2vec.Word2Vec(data.token, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "print ('Total time spent:', time.time() - begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we train the word2vec model, each word in our data set is associated with a (300,) vector. We recall that this vector is derived from the input-to-hidden-layer weights of the neural newwork model. Once we got this vecoter, we can plug in into the soft matrix to get the distribution of all the words. This distribution measures how similar (in terms of probability) the word is to the input. For example, we can find the similar words to 'friendly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('polite', 0.7394624352455139),\n",
       " ('courteous', 0.7326678037643433),\n",
       " ('friendliest', 0.6483472585678101),\n",
       " ('gracious', 0.6144683361053467),\n",
       " ('pleasant', 0.5970733165740967),\n",
       " ('nice', 0.5592578649520874),\n",
       " ('friendliness', 0.553424596786499),\n",
       " ('attentive', 0.5446797609329224),\n",
       " ('hospitable', 0.5388295650482178),\n",
       " ('tentative', 0.5190747976303101)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('friendly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each word has it's corresponding feature vectors of length 300. We than consider features of a review. A natural idea is that we compute the feature vecotor of each word in the review \n",
    "than take average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAverage(token):\n",
    "    \n",
    "    # Here 300 is the number of neurals in the hidden layer\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    \n",
    "    n_words = 0\n",
    "    # we have trained a word2vec model named 'model'\n",
    "    \n",
    "    # make a set of words we have learned in model\n",
    "    word_set = set(w2v_model.wv.index2word)\n",
    "    \n",
    "    for x in token:\n",
    "        if x in word_set:\n",
    "            n_words += 1\n",
    "            feature_vec += w2v_model[x]\n",
    "    \n",
    "    return np.divide(feature_vec,float(n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total time spent:', 211.63270497322083)\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "data['w2v_feature'] = data.token.apply(lambda token: getAverage(token))\n",
    "print ('Total time spent:', time.time() - begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we should be very careful, according to the algorithm of word2vec if the words are not used 'often' enough the features will be all 'None'. For example, in the following data the w2v_feature is all null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cool                                                           0\n",
       "funny                                                          0\n",
       "text           店堂不大，我到的早，周末中午第一拨客人，随后陆续来了不少中国人外国人，拖家带口的，小情侣，嗯...\n",
       "stars                                                          4\n",
       "token          [店堂不大，我到的早，周末中午第一拨客人，随后陆续来了不少中国人外国人，拖家带口的，小情侣，...\n",
       "w2v_feature    [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...\n",
       "Name: 12, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with this issue, we just drop all the data points with null features. (There are actually 83 of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ix = []\n",
    "for i in range(data.shape[0]):\n",
    "    if np.any(np.isnan(data.w2v_feature.iloc[i])): ix.append(i)\n",
    "data = data.drop(ix, axis= 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156607, 6)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can save the w2v_model \n",
    "w2v_model.save('./w2v_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! We are now ready to fit some machine learning algorithm on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# transform the feature into the form that sklearn accept\n",
    "feature = np.array([x for x in data.w2v_feature])\n",
    "X_train,  X_test, y_train, y_test = train_test_split(feature, data['stars'], test_size = 0.3, random_state = 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "n_trees = [100]\n",
    "m_depth = [10,15,20]\n",
    "clf = RandomForestClassifier()\n",
    "grid = GridSearchCV(estimator = clf, param_grid= dict(n_estimators = n_trees, max_depth = m_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total time spent:', datetime.timedelta(0, 3010, 277790))\n"
     ]
    }
   ],
   "source": [
    "begin = datetime.datetime.now()\n",
    "grid.fit(X_train, y_train)\n",
    "print ('Total time spent:', datetime.datetime.now() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'max_depth': 15}\n"
     ]
    }
   ],
   "source": [
    "print grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, max_depth=15)\n",
    "w2v_model = forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_predict = w2v_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52929783113040885"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Accuracy\n",
    "np.sum([w2v_predict == y_test])/float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general accuracy is not that impressive. But if we take a detailed of the result we can see that among the good restaurants, the accuracy is 94%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94393311576399841"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([(w2v_predict >= 4)& (y_test >= 4)])/float(np.sum(y_test >= 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056066884236001611"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict as bad rating while it is actually good rating\n",
    "np.sum([(w2v_predict < 4)& (y_test >= 4)])/float(np.sum(y_test >= 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054036781935667119"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage that 4 stars predicted as 3 stars\n",
    "np.sum([(w2v_predict == 3)& (y_test == 4)])/float(np.sum(y_test ==4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57437407952871866"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage that 3 stars predicted as 4 stars\n",
    "np.sum([(w2v_predict == 4)& (y_test == 3)])/float(np.sum(y_test==3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive or Negative\n",
    "Lets reduce the problem into binary classification, we divide ratings into 'good' (stars = 4,5) and 'bad' (stars = 1,2,3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cool</th>\n",
       "      <th>funny</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>token</th>\n",
       "      <th>w2v_feature</th>\n",
       "      <th>two_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This place is a gem. We had friendly attentive...</td>\n",
       "      <td>5</td>\n",
       "      <td>[place, gem, friendly, attentive, service, foo...</td>\n",
       "      <td>[0.23933, -0.652453, 0.746594, 0.98832, 1.2323...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This is perhaps the closest pho restaurant to ...</td>\n",
       "      <td>5</td>\n",
       "      <td>[perhaps, closest, pho, restaurant, port, cred...</td>\n",
       "      <td>[0.485038, 0.363754, 0.0717848, 0.172526, 0.95...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Happened to stumble upon this little quaint re...</td>\n",
       "      <td>4</td>\n",
       "      <td>[happened, stumble, upon, little, quaint, rest...</td>\n",
       "      <td>[-0.629566, -0.113229, 0.406629, 0.274055, 0.3...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cool  funny                                               text  stars  \\\n",
       "0     0      0  This place is a gem. We had friendly attentive...      5   \n",
       "1     0      0  This is perhaps the closest pho restaurant to ...      5   \n",
       "2     0      0  Happened to stumble upon this little quaint re...      4   \n",
       "\n",
       "                                               token  \\\n",
       "0  [place, gem, friendly, attentive, service, foo...   \n",
       "1  [perhaps, closest, pho, restaurant, port, cred...   \n",
       "2  [happened, stumble, upon, little, quaint, rest...   \n",
       "\n",
       "                                         w2v_feature two_label  \n",
       "0  [0.23933, -0.652453, 0.746594, 0.98832, 1.2323...      good  \n",
       "1  [0.485038, 0.363754, 0.0717848, 0.172526, 0.95...      good  \n",
       "2  [-0.629566, -0.113229, 0.406629, 0.274055, 0.3...      good  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def good_or_bad(stars):\n",
    "    if stars >= 4: return 'good'\n",
    "    else: return 'bad'\n",
    "data['two_label'] = data.stars.apply(lambda stars: good_or_bad(stars))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train,  X_test, y_train, y_test = train_test_split(feature, data['two_label'], test_size = 0.3, random_state = 1024)\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth=15)\n",
    "binary_model = forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82721409871655704"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_predict = binary_model.predict(X_test)\n",
    "# bianry accuracy\n",
    "np.sum([binary_predict == y_test])/float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "As a benchmark, we use the method of TF-IDF method to do the analysis. The major difference is the way we extract features from words. In TF-IDF we use the freqency of the word appears in a review (TF) and other reviews(IDF) to get a 'feature' of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "count_vec = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "cv = count_vec.fit_transform(data.text)\n",
    "tfidf_feature = tfidf.fit_transform(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_feature, data['stars'], test_size = 0.3, random_state = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, max_depth=15)\n",
    "tfidf_model = forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39028229838109219"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_predict = tfidf_model.predict(X_test)\n",
    "# Accuracy\n",
    "np.sum([tfidf_predict == y_test])/float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Two stage learning\n",
    "Use word2vec to extract features, then run fully connected NN with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_size = 300\n",
    "n_node_hl1 = 100\n",
    "n_node_hl2 = 100\n",
    "n_class = 5\n",
    "batch_size = 200\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholers\n",
    "x = tf.placeholder('float', [None, feature_size])\n",
    "\n",
    "# should assign shape?\n",
    "y = tf.placeholder('float', [None, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NN_model(data):\n",
    "    # Weights and bias of each layer\n",
    "    \n",
    "    hl1 = {'weights': tf.Variable(tf.random_normal([feature_size, n_node_hl1])),\n",
    "            'bias': tf.Variable(tf.random_normal([n_node_hl1])) }\n",
    "    hl2 = {'weights': tf.Variable(tf.random_normal([n_node_hl1, n_node_hl2])),\n",
    "            'bias': tf.Variable(tf.random_normal([n_node_hl2])) }\n",
    "\n",
    "    output_layer = { 'weights': tf.Variable(tf.random_normal([n_node_hl2, n_class])),\n",
    "                'bias' : tf.Variable(tf.random_normal([n_class]))}\n",
    "    \n",
    "    # Output of each layer\n",
    "    # Relu((x*W + bias))\n",
    "    s1 = tf.add(tf.matmul(data, hl1['weights']),hl1['bias'])\n",
    "    a1 = tf.nn.relu(s1)\n",
    "    \n",
    "    s2 = tf.add(tf.matmul(a1, hl2['weights']), hl2['bias'])\n",
    "    a2 = tf.nn.relu(s2)\n",
    "    \n",
    "    output = tf.add(tf.matmul(a2, output_layer['weights']), output_layer['bias'])\n",
    "    \n",
    "    #return tf.reshape(tf.cast(tf.argmax(output,1),'float'), [batch_size, 1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NN_model(x,y, n_epoch):\n",
    "    prediction = NN_model(x)\n",
    "    cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits = prediction, labels=y))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # cut into batches\n",
    "        i = 0\n",
    "        while i < len(X_train):\n",
    "            start = i\n",
    "            end = i + batch_size\n",
    "            batch_X = X_train[start: end]\n",
    "            \n",
    "            # the original y is single numeric labels, we transform into one hot \n",
    "            # but the out put of tf.one_hot is a tensor node, so we need to sess.run(batch_y)\n",
    "            batch_y = \\\n",
    "            tf.one_hot(np.array(y_train[start: end]-1),5,on_value=1.0, off_value=0.0)\n",
    "        \n",
    "            _, c = sess.run([optimizer, cost],feed_dict={x: batch_X, y:sess.run(batch_y)})\n",
    "            \n",
    "            epoch_loss += c\n",
    "            \n",
    "            i += batch_size\n",
    "        print  ('Epoch:', epoch, 'loss:', epoch_loss)\n",
    "    \n",
    "    \n",
    "    correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    \n",
    "    one_hot_y_test = tf.one_hot(np.array(y_test-1),5,on_value=1.0, off_value=0.0)\n",
    "    print ('Accuracy', accuracy.eval(feed_dict={x: X_test, y: sess.run(one_hot_y_test)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 0, 'loss:', 7566256.7371826172)\n",
      "('Epoch:', 1, 'loss:', 2794056.434387207)\n",
      "('Epoch:', 2, 'loss:', 1613582.7296142578)\n",
      "('Epoch:', 3, 'loss:', 990616.95877075195)\n",
      "('Epoch:', 4, 'loss:', 604009.03515625)\n",
      "('Epoch:', 5, 'loss:', 373846.35391998291)\n",
      "('Epoch:', 6, 'loss:', 261112.11938858032)\n",
      "('Epoch:', 7, 'loss:', 208930.66054153442)\n",
      "('Epoch:', 8, 'loss:', 183264.86547851562)\n",
      "('Epoch:', 9, 'loss:', 167344.09373474121)\n",
      "('Accuracy', 0.48677182)\n",
      "('Total time spent:', 10487.929569005966)\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "train_NN_model(x,y,n_epoch)\n",
    "print ('Total time spent:', time.time() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "n_node_hl1 = 50\n",
    "n_node_hl2 = 50\n",
    "n_class = 5\n",
    "batch_size = 3000\n",
    "n_epoch = 10\n",
    "Acc: 0.40\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "feature_size = 100\n",
    "n_node_hl1 = 50\n",
    "n_node_hl2 = 50\n",
    "n_class = 5\n",
    "batch_size = 3000\n",
    "n_epoch = 15\n",
    "Loss: 1400282\n",
    "Acc = 0.41\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "feature_size = 300\n",
    "n_node_hl1 = 100\n",
    "n_node_hl2 = 100\n",
    "n_class = 5\n",
    "batch_size = 3000\n",
    "n_epoch = 10\n",
    "Acc = 0.435\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "feature_size = 300     \n",
    "n_node_hl1 = 100\n",
    "n_node_hl2 = 100\n",
    "n_class = 5\n",
    "batch_size = 500\n",
    "n_epoch = 10\n",
    "Loss : 352956\n",
    "Acc = 0.459\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "feature_size = 300\n",
    "n_node_hl1 = 100\n",
    "n_node_hl2 = 100\n",
    "n_class = 5\n",
    "batch_size = 500\n",
    "n_epoch = 15\n",
    "Loss = 176313\n",
    "Acc = 0.44\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "feature_size = 300\n",
    "n_node_hl1 = 100\n",
    "n_node_hl2 = 100\n",
    "n_class = 5\n",
    "batch_size = 200\n",
    "n_epoch = 10\n",
    "Loss = 167344\n",
    "Acc = 0.48\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
